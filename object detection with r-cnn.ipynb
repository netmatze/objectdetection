{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28b86385-11d9-49cc-8c4d-7cdbc9d67c86",
   "metadata": {},
   "source": [
    "## create a r-cnn model for object detection and train it with the Aquarium datase\n",
    "\n",
    "### https://public.roboflow.com/object-detection/aquarium/2#\n",
    "\n",
    "### https://chatgpt.com/c/5f97037a-3b6c-4521-bc2f-8070b6ee8b50\n",
    "\n",
    "## \"Aquarium.coco\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cae160b-30ab-44d7-ad25-c41e3698bf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install roboflow\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"fqSAzeINBX9oPfwLixDj\")\n",
    "project = rf.workspace(\"brad-dwyer\").project(\"aquarium\")\n",
    "version = project.version(2)\n",
    "dataset = version.download(\"coco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c39ffeb-329c-4ab5-85e8-cb3d9847d927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torchvision.transforms as T\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "348d9591-ef13-4452-9661-1fb1e2c3f96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "class RoboFlowDataset(Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"train\"))))\n",
    "        self.annotations = json.load(open(os.path.join(root, \"train/_annotations.coco.json\")))\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"train\", self.imgs[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        ann = [a for a in self.annotations['annotations'] if a['image_id'] == idx]\n",
    "        boxes = [a['bbox'] for a in ann]\n",
    "        labels = [a['category_id'] for a in ann]\n",
    "        \n",
    "        # Convert to tensors\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        \n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "def get_transform(train):    \n",
    "    if train:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((512, 512)),  # Resize to 512x512\n",
    "            transforms.RandomHorizontalFlip(),  # Random horizontal flip for augmentation\n",
    "            transforms.ToTensor(),  # Convert image to PyTorch tensor\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalize to the same mean and std used by pretrained models\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "    else:\n",
    "        return transforms.Compose([\n",
    "            transforms.Resize((512, 512)),  # Resize to 512x512\n",
    "            transforms.ToTensor(),  # Convert image to PyTorch tensor\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406],  # Normalize to the same mean and std used by pretrained models\n",
    "                                 std=[0.229, 0.224, 0.225]),\n",
    "        ])\n",
    "\n",
    "\n",
    "#def get_transform(train):\n",
    "#    transforms = []\n",
    "#    transforms.append(T.ToTensor())\n",
    "#    if train:\n",
    "#        transforms.append(T.RandomHorizontalFlip(0.5))\n",
    "#    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c834636c-90fe-4859-a790-55ae37bfa014",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1),  # [16, H/2, W/2]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),  # [16, H/4, W/4]\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),  # [32, H/8, W/8]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),  # [32, H/16, W/16]\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),  # [64, H/32, W/32]\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),  # [64, H/64, W/64]\n",
    "        )\n",
    "        self.out_channels = 64\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.features(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57682197-ee2f-4e6d-ae7b-623728284831",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DetectionHead(nn.Module):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super(DetectionHead, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.cls_head = nn.Sequential(\n",
    "            nn.Linear(in_channels * 7 * 7, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "        self.reg_head = nn.Sequential(\n",
    "            nn.Linear(in_channels * 7 * 7, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 4)  # 4 for bounding box regression\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.flatten(start_dim=1)\n",
    "        cls_logits = self.cls_head(x)\n",
    "        bbox_reg = self.reg_head(x)\n",
    "        return cls_logits, bbox_reg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0bc4c050-d979-43db-81a5-6a608516eb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRCNN(nn.Module):\n",
    "    def __init__(self, backbone, num_classes):\n",
    "        super(SimpleRCNN, self).__init__()\n",
    "        self.backbone = backbone\n",
    "        self.head = DetectionHead(backbone.out_channels, num_classes)\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        features = self.backbone(images)\n",
    "        pooled_features = nn.functional.adaptive_max_pool2d(features, (7, 7))\n",
    "\n",
    "        cls_logits, bbox_reg = self.head(pooled_features)\n",
    "        \n",
    "        if self.training:\n",
    "            # targets is a list of dictionaries            \n",
    "            print(f\"cls_logits: {cls_logits} - shape: {cls_logits.shape}\")\n",
    "            for target in targets:\n",
    "                print(f\"lables: {target['labels']} - shape: {target['labels'].shape}\")\n",
    "            \n",
    "            # Compute losses\n",
    "            loss_cls = sum(nn.CrossEntropyLoss()(cls_logits[i], target[\"labels\"]) for i, target in enumerate(targets))\n",
    "            loss_bbox = sum(nn.MSELoss()(bbox_reg[i], target[\"boxes\"]) for i, target in enumerate(targets))\n",
    "            losses = {\"loss_cls\": loss_cls, \"loss_bbox\": loss_bbox}\n",
    "            return losses            \n",
    "\n",
    "        return cls_logits, bbox_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff172e91-9f82-47cd-bc16-1a0785a992ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Anzahl der Klassen im Roboflow-Dataset (einschließlich Hintergrund)\n",
    "num_classes = len(json.load(open('Aquarium.coco/train/_annotations.coco.json'))['categories']) + 1\n",
    "\n",
    "dataset = RoboFlowDataset('Aquarium.coco', get_transform(train=True))\n",
    "dataset_test = RoboFlowDataset('Aquarium.coco', get_transform(train=False))\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x))) #, num_workers=4, collate_fn=lambda x: tuple(zip(*x)))\n",
    "data_loader_test = DataLoader(dataset_test, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x))) #, num_workers=4, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "\n",
    "#for images, targets in data_loader:\n",
    "#    print(images)\n",
    "#    print(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d4302921-f628-472e-b796-7bb1fb3a34db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running for 2 epochs\n",
      "Started Training\n",
      "Run: 0 - After targets\n",
      "cls_logits: tensor([[ 0.0302, -0.0080, -0.0271,  0.0696, -0.0113, -0.0542, -0.0287,  0.0647,\n",
      "          0.0115],\n",
      "        [ 0.0176, -0.0194, -0.0162,  0.0824,  0.0014, -0.0327, -0.0335,  0.0325,\n",
      "         -0.0050]], device='cuda:0', grad_fn=<AddmmBackward0>) - shape: torch.Size([2, 9])\n",
      "lables: tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2], device='cuda:0') - shape: torch.Size([10])\n",
      "lables: tensor([1, 5, 1, 1, 1, 1, 1, 1, 1], device='cuda:0') - shape: torch.Size([9])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch (got input: [9], target: [10])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 23\u001b[0m\n\u001b[0;32m     20\u001b[0m targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcounter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - After targets\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m losses \u001b[38;5;241m=\u001b[39m model(images, targets)        \n\u001b[0;32m     24\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(loss \u001b[38;5;28;01mfor\u001b[39;00m loss \u001b[38;5;129;01min\u001b[39;00m losses\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcounter\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - After loss\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mE:\\Programme\\anaconda\\envs\\pytorch_cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Programme\\anaconda\\envs\\pytorch_cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[30], line 20\u001b[0m, in \u001b[0;36mSimpleRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlables: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Compute losses\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m loss_cls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()(cls_logits[i], target[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i, target \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(targets))\n\u001b[0;32m     21\u001b[0m loss_bbox \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(nn\u001b[38;5;241m.\u001b[39mMSELoss()(bbox_reg[i], target[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i, target \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(targets))\n\u001b[0;32m     22\u001b[0m losses \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_cls\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss_cls, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_bbox\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss_bbox}\n",
      "Cell \u001b[1;32mIn[30], line 20\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlables: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Compute losses\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m loss_cls \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()(cls_logits[i], target[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i, target \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(targets))\n\u001b[0;32m     21\u001b[0m loss_bbox \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(nn\u001b[38;5;241m.\u001b[39mMSELoss()(bbox_reg[i], target[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m i, target \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(targets))\n\u001b[0;32m     22\u001b[0m losses \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_cls\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss_cls, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss_bbox\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss_bbox}\n",
      "File \u001b[1;32mE:\\Programme\\anaconda\\envs\\pytorch_cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Programme\\anaconda\\envs\\pytorch_cuda\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Programme\\anaconda\\envs\\pytorch_cuda\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1179\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m   1180\u001b[0m                            ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[0;32m   1181\u001b[0m                            label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing)\n",
      "File \u001b[1;32mE:\\Programme\\anaconda\\envs\\pytorch_cuda\\Lib\\site-packages\\torch\\nn\\functional.py:3059\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3057\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3058\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3059\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: size mismatch (got input: [9], target: [10])"
     ]
    }
   ],
   "source": [
    "backbone = SimpleCNN()\n",
    "model = SimpleRCNN(backbone, num_classes)\n",
    "model.to(device)\n",
    "\n",
    "# Optimierer definieren\n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "\n",
    "# Training\n",
    "num_epochs = 2\n",
    "print(f\"Running for {num_epochs} epochs\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    print(\"Started Training\")\n",
    "    counter = 0\n",
    "    for images, targets in data_loader:        \n",
    "        images = torch.stack(images).to(device)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "        print(f\"Run: {counter} - After targets\")\n",
    "        \n",
    "        losses = model(images, targets)        \n",
    "        loss = sum(loss for loss in losses.values())\n",
    "        print(f\"Run: {counter} - After loss\")\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #if counter % 100 == 0:\n",
    "        print(f\"Epoch: {epoch} - Run: {counter}\")\n",
    "        counter += counter\n",
    "    \n",
    "    # Lernrate anpassen\n",
    "    lr_scheduler.step()\n",
    "    \n",
    "    print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50485ccd-ca49-4958-bc86-4f3f6edee97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"simple_rcnn.pth\")\n",
    "\n",
    "# Um das Modell später zu laden\n",
    "backbone = SimpleCNN()\n",
    "model = SimpleRCNN(backbone, num_classes)\n",
    "model.load_state_dict(torch.load(\"simple_rcnn.pth\"))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc2a454-b8c4-4e9e-b7db-145db76af7ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.transforms import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "\n",
    "# Laden Sie das Modell\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "backbone = SimpleCNN()\n",
    "model = SimpleRCNN(backbone, num_classes)  # num_classes entspricht der Anzahl der Klassen in Ihrem Dataset\n",
    "model.load_state_dict(torch.load(\"simple_rcnn.pth\"))\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a6bdae-481f-4a5b-99a0-c29616218e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_tensor = F.to_tensor(image).unsqueeze(0)  # Fügen Sie eine Batch-Dimension hinzu\n",
    "    return image, image_tensor\n",
    "\n",
    "image_path = 'path_to_your_image.jpg'\n",
    "image, image_tensor = load_image(image_path)\n",
    "image_tensor = image_tensor.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c153616-e061-409a-bdfe-72cabbfcf31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    cls_logits, bbox_reg = model(image_tensor)\n",
    "\n",
    "# Konvertieren Sie die Ergebnisse in numpy Arrays und bringen Sie sie auf die CPU\n",
    "cls_logits = cls_logits.cpu().numpy()\n",
    "bbox_reg = bbox_reg.cpu().numpy()\n",
    "\n",
    "# Wählen Sie die Klasse mit der höchsten Wahrscheinlichkeit\n",
    "predicted_class = np.argmax(cls_logits, axis=1)\n",
    "predicted_boxes = bbox_reg\n",
    "\n",
    "# Umwandeln der Bounding Box-Koordinaten\n",
    "predicted_boxes[:, 0] = predicted_boxes[:, 0] * image.width\n",
    "predicted_boxes[:, 1] = predicted_boxes[:, 1] * image.height\n",
    "predicted_boxes[:, 2] = predicted_boxes[:, 2] * image.width\n",
    "predicted_boxes[:, 3] = predicted_boxes[:, 3] * image.height\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86deb58-9bb4-431a-91a2-ecac07005ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(image, boxes, labels):\n",
    "    fig, ax = plt.subplots(1)\n",
    "    ax.imshow(image)\n",
    "    \n",
    "    for box, label in zip(boxes, labels):\n",
    "        # Zeichnen Sie die Bounding Box\n",
    "        rect = patches.Rectangle((box[0], box[1]), box[2], box[3], linewidth=2, edgecolor='r', facecolor='none')\n",
    "        ax.add_patch(rect)\n",
    "        # Textbeschriftung hinzufügen\n",
    "        plt.text(box[0], box[1], f'Class: {label}', bbox=dict(facecolor='yellow', alpha=0.5))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "plot_results(image, predicted_boxes, predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42def8d8-bc57-4614-a42e-0680084190b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5238e58-cfff-40a2-be18-799988988792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b378532b-cd45-418a-837a-292e4e20f769",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927078ae-25ca-40a6-bbc3-268c93e5d369",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
